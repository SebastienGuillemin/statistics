get_ipython().run_line_magic("matplotlib", " widget")
import psycopg2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn.manifold import TSNE
from sklearn_extra.cluster import KMedoids
from sklearn.metrics import silhouette_score
import pandas as pd
import gower
from tqdm.notebook import tqdm
from yellowbrick.cluster import SilhouetteVisualizer


# Loading data
conn = psycopg2.connect(database="full_STUPS",
                        user="postgres",
                        host='localhost',
                        password="postgres",
                        port=5432)
cur = conn.cursor()
cur.execute( '''select ep.*, id_lot
                from echantillon_propriete ep 
                inner join echantillon e on e.id = ep.id 
                inner join composition c on c.id = e.id_composition 
                inner join lot_complet lc on (lc.e1 = c.id or lc.e2 = c.id)
                order by id_lot''')
samples = cur.fetchall()
conn.commit()
conn.close()
colnames = [desc[0] for desc in cur.description]


# Subsampling data
subsample = False

X = pd.DataFrame(list(samples), columns=colnames) 
Y = X['id_lot']

X = X.drop(columns=['id', 'num_echantillon', 'id_lot'])

if subsample:
    X = X.iloc[:3750, :]
    
print(f'Samples shape : {X.shape}')


# Computing distance matrix between all samples
dist_matrix = gower.gower_matrix(X)
print(f'Distance matrix size : {dist_matrix.size}')


# Computing t-SNE
tsne = TSNE(n_components=3,
            perplexity=50,
            init='random',
            metric='precomputed')
embedding = tsne.fit_transform(dist_matrix)


# Printing t-SNE results
fig = plt.figure(figsize=(5, 5))
ax = Axes3D(fig, auto_add_to_figure=False)
fig.add_axes(ax)

# plot
sc = ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], s=10, marker='o',  alpha=0.5)


# Computing different clustering using kmenoids
# with different number of clusters 
labels_results = []
wcss = []
min_clusters = 2
nb_clusters = 100
nb_clusters_range = range(min_clusters, nb_clusters + min_clusters)

for i in tqdm(nb_clusters_range):
    model = KMedoids(n_clusters=i, init='k-medoids++').fit(embedding)
    
    cluster_labels = model.labels_
    silhouette_avg = silhouette_score(embedding, cluster_labels)
    
    labels_results.append(cluster_labels)
    wcss.append(silhouette_avg)


model = KMedoids(n_clusters=1968, init='k-medoids++').fit(embedding)
cluster_labels = model.labels_


# Printing curve(s)
x = nb_clusters_range
#d_wcss = np.gradient(wcss)
#d2_wcss = np.gradient(wcss)

fig = plt.figure(figsize=(8, 8))
plt.plot(x, wcss)
#plt.plot(x, d_wcss)
#plt.plot(x, d2_wcss)
plt.ylabel('WCSS')

plt.grid(visible=True, alpha=0.6)
plt.show()


# Finding the optimal cluster number
index_cluster = np.argmax(wcss)

#visualizer = SilhouetteVisualizer(KMedoids(n_clusters=index_cluster, init='k-medoids++'), colors='yellowbrick')
#visualizer.fit(embedding)
#visualizer.show()

optimal_clusters_number = index_cluster + 1 # Indexes start at 0.
print(f'Optimal cluster number : {optimal_clusters_number}.')


# Printing the cluster according to the optimal cluster number
embedding_df = pd.DataFrame({'x': embedding[:, 0],
                             'y': embedding[:, 1],
                             'z': embedding[:, 2],
                             'label': cluster_labels})

# axes instance
fig = plt.figure(figsize=(8, 8))
ax = Axes3D(fig, auto_add_to_figure=False)

ax.set_xlabel('Embedding 1')
ax.set_ylabel('Embedding 2')
ax.set_zlabel('Embedding 3')
fig.add_axes(ax)

# get colormap from seaborn
cmap = ListedColormap(sns.color_palette("husl", optimal_clusters_number).as_hex())

# plot
sc = ax.scatter(embedding_df.loc[:, 'x'], embedding_df.loc[:, 'y'], embedding_df.loc[:, 'z'], s=20, c=embedding_df.loc[:, 'label'], marker='o', cmap=cmap, alpha=0.5)


# Comparing labels
print(embedding_df.shape)
print(Y.shape)
